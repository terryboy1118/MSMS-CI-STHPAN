args: Namespace(random_seed=2023, dset_pretrain='stock', context_points=512, target_points=1, batch_size=1, num_workers=10, scaler='standard', features='M', market='NYSE', patch_len=12, stride=12, revin=1, ci=1, graph=1, rel_type=8, k=20, n_layers=3, n_heads=16, d_model=128, d_ff=256, dropout=0.2, head_dropout=0.0, mask_ratio=0.4, n_epochs_pretrain=100, lr=0.0001, pretrained_model_id=1, model_type='based_model')
use GPU: 0
#datautilis tickers_fname: NYSE_tickers_qualify_dr-0.98_min-5_smooth.csv
#tickers selected: 1737
#single EOD data shape: (756, 5)
#eod_data shape: (1737, 756, 5)
#masks shape: (1737, 756)
#ground_truth shape: (1737, 756)
#base_price shape: (1737, 756)
#data_stamp shape: (1737, 756, 4)
#tickers selected: 1737
#single EOD data shape: (764, 5)
#eod_data shape: (1737, 764, 5)
#masks shape: (1737, 764)
#ground_truth shape: (1737, 764)
#base_price shape: (1737, 764)
#data_stamp shape: (1737, 764, 4)
#tickers selected: 1737
#single EOD data shape: (749, 5)
#eod_data shape: (1737, 749, 5)
#masks shape: (1737, 749)
#ground_truth shape: (1737, 749)
#base_price shape: (1737, 749)
#data_stamp shape: (1737, 749, 4)
number of patches: 42
number of model params: 1872268
suggested_lr: 0.00035938136638046257
#datautilis tickers_fname: NYSE_tickers_qualify_dr-0.98_min-5_smooth.csv
#tickers selected: 1737
#single EOD data shape: (756, 5)
#eod_data shape: (1737, 756, 5)
#masks shape: (1737, 756)
#ground_truth shape: (1737, 756)
#base_price shape: (1737, 756)
#data_stamp shape: (1737, 756, 4)
#tickers selected: 1737
#single EOD data shape: (764, 5)
#eod_data shape: (1737, 764, 5)
#masks shape: (1737, 764)
#ground_truth shape: (1737, 764)
#base_price shape: (1737, 764)
#data_stamp shape: (1737, 764, 4)
#tickers selected: 1737
#single EOD data shape: (749, 5)
#eod_data shape: (1737, 749, 5)
#masks shape: (1737, 749)
#ground_truth shape: (1737, 749)
#base_price shape: (1737, 749)
#data_stamp shape: (1737, 749, 4)
number of patches: 42
number of model params: 1872268
          epoch     train_loss     valid_loss           time
Better model found at epoch 0 with valid_loss value: 0.5644962371341766.
              0       0.811277       0.564496          01:46
Better model found at epoch 1 with valid_loss value: 0.37370167081318206.
              1       0.535254       0.373702          01:46
Better model found at epoch 2 with valid_loss value: 0.2678615025111607.
              2       0.374783       0.267862          01:46
Better model found at epoch 3 with valid_loss value: 0.2186261737157428.
              3       0.294700       0.218626          01:46
Better model found at epoch 4 with valid_loss value: 0.1834249496459961.
              4       0.256385       0.183425          01:46
Better model found at epoch 5 with valid_loss value: 0.17247103131006633.
              5       0.233130       0.172471          01:46
Better model found at epoch 6 with valid_loss value: 0.16649014609200613.
              6       0.216984       0.166490          01:46
Better model found at epoch 7 with valid_loss value: 0.15379696800595238.
              7       0.204272       0.153797          01:46
Better model found at epoch 8 with valid_loss value: 0.14876232449970547.
              8       0.192560       0.148762          01:46
Better model found at epoch 9 with valid_loss value: 0.14475480336991567.
              9       0.183869       0.144755          01:46
Better model found at epoch 10 with valid_loss value: 0.14352532038612972.
             10       0.175082       0.143525          01:46
Better model found at epoch 11 with valid_loss value: 0.13479879167344835.
             11       0.167239       0.134799          01:46
Better model found at epoch 12 with valid_loss value: 0.12899990687294613.
             12       0.160570       0.129000          01:46
Better model found at epoch 13 with valid_loss value: 0.1278298090374659.
             13       0.154768       0.127830          01:46
Better model found at epoch 14 with valid_loss value: 0.12407273337954566.
             14       0.149302       0.124073          01:46
Better model found at epoch 15 with valid_loss value: 0.12026583202301509.
             15       0.144501       0.120266          01:46
Better model found at epoch 16 with valid_loss value: 0.11935629920353966.
             16       0.140179       0.119356          01:46
Better model found at epoch 17 with valid_loss value: 0.1149705099681067.
             17       0.135216       0.114971          01:46
Better model found at epoch 18 with valid_loss value: 0.11227563827756852.
             18       0.130871       0.112276          01:46
Better model found at epoch 19 with valid_loss value: 0.10767104890611437.
             19       0.125194       0.107671          01:46
Better model found at epoch 20 with valid_loss value: 0.10454973341926696.
             20       0.117246       0.104550          01:46
Better model found at epoch 21 with valid_loss value: 0.09856005320473323.
             21       0.110988       0.098560          01:46
Better model found at epoch 22 with valid_loss value: 0.09682041501242017.
             22       0.105850       0.096820          01:46
Better model found at epoch 23 with valid_loss value: 0.09287414853535002.
             23       0.101059       0.092874          01:46
Better model found at epoch 24 with valid_loss value: 0.08967963476029653.
             24       0.097869       0.089680          01:46
Better model found at epoch 25 with valid_loss value: 0.0877670030745249.
             25       0.095138       0.087767          01:46
Better model found at epoch 26 with valid_loss value: 0.08586751847040086.
             26       0.092431       0.085868          01:46
Better model found at epoch 27 with valid_loss value: 0.08289154749068003.
             27       0.089570       0.082892          01:46
Better model found at epoch 28 with valid_loss value: 0.0807944933573405.
             28       0.086790       0.080794          01:46
Better model found at epoch 29 with valid_loss value: 0.08067836458720858.
             29       0.084741       0.080678          01:46
Better model found at epoch 30 with valid_loss value: 0.07845038459414527.
             30       0.082716       0.078450          01:46
Better model found at epoch 31 with valid_loss value: 0.07784178900340247.
             31       0.080959       0.077842          01:46
Better model found at epoch 32 with valid_loss value: 0.07560453717670744.
             32       0.079154       0.075605          01:46
Better model found at epoch 33 with valid_loss value: 0.07548247443305121.
             33       0.077014       0.075482          01:46
             34       0.074619       0.077137          01:46
Better model found at epoch 35 with valid_loss value: 0.07542682829357329.
             35       0.072887       0.075427          01:46
Better model found at epoch 36 with valid_loss value: 0.07517757113017733.
             36       0.071686       0.075178          01:46
             37       0.070579       0.078136          01:46
             38       0.069625       0.080233          01:46
             39       0.068731       0.078623          01:46
             40       0.067738       0.081024          01:46
             41       0.066925       0.083627          01:46
             42       0.066073       0.084120          01:46
             43       0.065376       0.085560          01:46
             44       0.064609       0.085685          01:46
             45       0.063907       0.087646          01:46
             46       0.063183       0.090239          01:46
             47       0.062598       0.089123          01:46
             48       0.061958       0.089015          01:46
             49       0.061314       0.089634          01:46
             50       0.060736       0.090309          01:46
             51       0.060175       0.090360          01:46
             52       0.059753       0.090954          01:46
             53       0.059243       0.091651          01:46
             54       0.058908       0.091457          01:46
             55       0.058534       0.092768          01:46
             56       0.058216       0.093026          01:46
             57       0.057958       0.095207          01:46
             58       0.057598       0.095790          01:46
             59       0.057372       0.094804          01:46
             60       0.057126       0.096884          01:46
             61       0.056957       0.093994          01:46
             62       0.056651       0.096418          01:46
             63       0.056516       0.094713          01:46
             64       0.056258       0.096618          01:46
             65       0.056067       0.097948          01:46
             66       0.055846       0.097284          01:46
             67       0.055663       0.100224          01:46
             68       0.055611       0.100063          01:46
             69       0.055417       0.100163          01:46
             70       0.055300       0.101323          01:46
             71       0.055126       0.098653          01:46
             72       0.055022       0.098459          01:46
             73       0.054848       0.099608          01:46
             74       0.054775       0.101270          01:46
             75       0.054713       0.099178          01:46
             76       0.054612       0.100573          01:46
             77       0.054540       0.100925          01:46
             78       0.054409       0.100953          01:46
             79       0.054355       0.100720          01:46
             80       0.054256       0.098292          01:46
             81       0.054185       0.101206          01:46
             82       0.054120       0.100372          01:46
             83       0.054094       0.099423          01:46
             84       0.054023       0.100314          01:46
             85       0.053935       0.099808          01:46
             86       0.053902       0.100635          01:46
             87       0.053856       0.099970          01:46
             88       0.053815       0.100357          01:46
             89       0.053824       0.100396          01:46
             90       0.053814       0.099468          01:46
             91       0.053760       0.100395          01:46
             92       0.053743       0.101188          01:46
             93       0.053740       0.100725          01:46
             94       0.053704       0.100690          01:46
             95       0.053698       0.100508          01:46
             96       0.053657       0.100372          01:46
             97       0.053664       0.100258          01:46
             98       0.053704       0.100673          01:46
             99       0.053666       0.100175          01:46
itr 0 pretraining completed！ time: 14633s
